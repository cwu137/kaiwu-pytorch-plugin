{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd5ac77",
   "metadata": {},
   "source": [
    "### 堆叠受限玻尔兹曼机（RBM）原理与代码实现总结\n",
    "---\n",
    "#### **一、RBM原理概述**\n",
    "受限玻尔兹曼机（Restricted Boltzmann Machine）是一种基于能量的概率图模型，由可见层（Visible Layer）和隐层（Hidden Layer）组成，层内无连接，层间全连接。其核心是通过无监督学习学习数据的潜在特征分布。\n",
    "##### **1. 模型结构**\n",
    "- **可见层（v）**：输入数据的显式表示（如像素值）。\n",
    "- **隐层（h）**：提取的潜在特征。\n",
    "- **权重矩阵（W）**：连接可见层与隐层的权重。\n",
    "- **偏置**：可见层偏置（b）和隐层偏置（c）。\n",
    "##### **2. 能量函数与概率分布**\n",
    "RBM的能量函数定义为：\n",
    "$$\n",
    "E(\\mathbf{v}, \\mathbf{h}) = -\\mathbf{v}^T W \\mathbf{h} - \\mathbf{b}^T \\mathbf{v} - \\mathbf{c}^T \\mathbf{h}\n",
    "$$\n",
    "联合概率分布通过玻尔兹曼分布给出：\n",
    "$$\n",
    "P(\\mathbf{v}, \\mathbf{h}) = \\frac{ e^{-E(\\mathbf{v}, \\mathbf{h})} }{Z}\n",
    "$$\n",
    "其中 $ Z $ 为配分函数（归一化因子）。可见层的边缘分布为：\n",
    "$$\n",
    "P(\\mathbf{v}) = \\sum_{\\mathbf{h}} P(\\mathbf{v}, \\mathbf{h})\n",
    "$$\n",
    "##### **3. 条件独立性**\n",
    "由于层内无连接，给定可见层时隐层条件独立，反之亦然：\n",
    "$$\n",
    "P(h_j=1|\\mathbf{v}) = \\sigma\\left(\\sum_i W_{ij} v_i + c_j\\right)\n",
    "$$\n",
    "$$\n",
    "P(v_i=1|\\mathbf{h}) = \\sigma\\left(\\sum_j W_{ij} h_j + b_i\\right)\n",
    "$$\n",
    "其中 $\\sigma(x) = \\frac{1}{1+e^{-x}}$ 为Sigmoid激活函数。\n",
    "##### **4. 训练目标**\n",
    "通过最大化似然函数学习参数（W, b, c）。目标函数为负对数似然：\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{\\mathbf{v}} \\log P(\\mathbf{v})\n",
    "$$\n",
    "采用对比散度（CD）算法近似梯度，更新规则为：\n",
    "$$\n",
    "\\Delta W_{ij} = \\epsilon \\left(\\langle v_i h_j \\rangle_{\\text{data}} - \\langle v_i h_j \\rangle_{\\text{recon}}\\right)\n",
    "$$\n",
    "其中 $\\epsilon$ 为学习率，$\\langle \\cdot \\rangle_{\\text{data}}$ 和 $\\langle \\cdot \\rangle_{\\text{recon}}$ 分别为数据分布和重构分布的期望。\n",
    "\n",
    "---\n",
    "#### **二、整体模块架构与训练模式**\n",
    "代码实现了基于PyTorch的深度信念网络(DBN)，采用分层架构设计，支持从无监督预训练到有监督学习的完整流程。\n",
    "\n",
    "- **模块架构**：\n",
    "  -  **DBNPretrainer**  \n",
    "      - 实现多层RBM的堆叠与逐层无监督预训练，提供特征提取接口。  \n",
    "  -  **AbstractSupervisedDBN**  \n",
    "      - 定义DBN监督学习的通用接口用于支持多模式训练策略，包括预训练、微调、分类器训练与预测等抽象方法。  \n",
    "  -  **PyTorchAbstractSupervisedDBN**  \n",
    "      - 基于`AbstractSupervisedDBN`，实现PyTorch下的通用工具方法封装，包括特征提取、分类器集成(逻辑回归，支持向量机以及随机森林等)等。\n",
    "  -  **PyTorchSupervisedDBNClassification**  \n",
    "      - 具体分类任务实现、微调网络构建以及训练。\n",
    "\n",
    "- **训练模式**：\n",
    "  1. 无监督模式: 仅预训练，用于特征提取\n",
    "  2. 分类器模式: 预训练 + 下游分类器训练\n",
    "  3. 微调网络模式: 预训练 + 网络反向传播微调\n",
    "   \n",
    "---\n",
    "#### **三、核心类功能和接口概述**\n",
    "\n",
    "##### **1. 核心类 `DBNPretrainer（无监督预训练DBN）`**\n",
    "- **关键参数**：\n",
    "  - `hidden_layers_structure`：隐层单元数（默认两层[100, 100]）\n",
    "  - `learning_rate_rbm`：RBM学习率（默认0.1）\n",
    "  - `n_epochs_rbm`：每层RBM训练轮数（默认10）\n",
    "  - `batch_size`：批大小（默认100）\n",
    "  - `verbose`：打印训练信息（默认True）\n",
    "  - `shuffle`：数据打乱（默认True）\n",
    "  - `drop_last`：是否丢弃最后不足batch的样本（默认False）\n",
    "  - `random_state`：随机种子\n",
    "- **设备支持**：自动选择GPU（`cuda`）或CPU\n",
    "- **核心方法**\n",
    "  - **预训练堆叠RBM (`fit` 方法)**\n",
    "  - **特征变换，逐层提取特征 (`transform` 方法)**\n",
    "  - **单层RBM训练 (`_train_rbm_layer` 方法)**\n",
    "      - **初始化优化器**：采用随机梯度下降（SGD）优化参数。\n",
    "      - **DataLoader处理批量数据**\n",
    "  - **单批次训练步骤 (`_train_batch` 方法)**\n",
    "    1. **正相（Positive Phase）**：计算隐层激活概率 $ P(\\mathbf{h}|\\mathbf{v}) $。\n",
    "    2. **负相（Negative Phase）**：通过模拟退火采样器（`SimulatedAnnealingOptimizer`）生成重构样本。\n",
    "    3. **目标函数**：最小化能量函数加权重衰减（L2正则化）。\n",
    "    4. **反向传播**：更新权重和偏置。\n",
    "  - **创建RBM层 (`_create_rbm_layer` 方法)**\n",
    "    - **初始化RBM**：使用 `RestrictedBoltzmannMachine` 定义可见层与隐层维度。\n",
    "\n",
    "##### **2. 核心类 `AbstractSupervisedDBN（抽象接口定义）`**\n",
    "- **关键参数**：\n",
    "  - `fine_tuning`：模式选择（默认False）\n",
    "  - `learning_rate`：微调学习率（默认0.1）\n",
    "  - `n_iter_backprop`：反向传播迭代次数（默认100）\n",
    "  - `l2_regularization`：L2正则化（默认1e-4）\n",
    "  - `activation_function`：激活函数（默认'sigmoid'）\n",
    "  - `dropout_p`：Dropout概率（默认0.0）\n",
    "##### **3. 核心类 `PyTorchAbstractSupervisedDBN（分类器模式具体实现，以及微调网络工具）`**\n",
    "- **关键参数**：\n",
    "  - `classifier_type`：支持多种分类器（默认逻辑回归）\n",
    "  - `clf_C`：正则化强度（默认1.0）\n",
    "  - `clf_iter`：迭代次数（默认100）\n",
    "##### **4. 核心类 `SupervisedDBNClassification（具体分类实现）`**\n",
    "- **微调网络构建**：使用预训练的权重来初始化(默认两层RBM，以及无Dropout层)\n",
    "  - 网络结构：输入层 → [线性层 + 激活函数 + Dropout] × N → 输出层\n",
    "  - 线性层：使用对应的RBM的权重初始化\n",
    "  - 输出层：随机初始化，在微调阶段学习\n",
    "\n",
    "- **训练策略**：\n",
    "  - 使用预训练权重初始化\n",
    "  - CrossEntropyLoss损失函数\n",
    "  - SGD优化器 + L2正则化\n",
    "  - 支持Dropout防止过拟合\n",
    "\n",
    "##### **5. 其他内容**\n",
    "\n",
    "- **数据加载 (`load_data` 方法)**\n",
    "\n",
    "    - 数据集：使用 `sklearn.datasets.load_digits`（8x8手写数字图像）。\n",
    "    - 增强：对原始图像进行上下左右平移，扩展数据集。\n",
    "\n",
    "- **训练过程可视化 (`_visualize_training_progress` 方法, 设置`plot_img=True`)**\n",
    "\n",
    "    - 权重与梯度：实时监控权重矩阵及其梯度变化。\n",
    "    - 生成样本：实时展示模型\"生成\"新样本的能力\n",
    "    - 重建样本：可视化重建误差的演变\n",
    "\n",
    "- **结果可视化 (`RBMVisualizer`类)**\n",
    "\n",
    "    - 训练后RBM权重可视化\n",
    "    - 分类任务结果: 混淆矩阵可视化\n",
    "    - 重建样本：训练完成后对测试图像进行编码-解码得到的重建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f0e9dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "seed = 42\n",
    "# PyTorch\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)       # 为当前GPU设置\n",
    "    torch.cuda.manual_seed_all(seed)   # 为所有GPU设置\n",
    "# Python\n",
    "random.seed(seed)\n",
    "# NumPy\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89bfdf9c-b6f5-41e8-8891-722220c7cb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Mode 1: Fine-tuning Network ===\n",
      "\n",
      "[DBN] Pre-training RBM layer 1/2: 64 -> 128\n",
      "[DBN] Pre-training start:\n",
      "jmean 0.039124jmax 0.124869\n",
      "hmean 0.050259hmax 0.099750\n",
      "Iteration 1, Average Loss: 0.074155\n",
      "Iteration 6, Average Loss: 2.063396\n",
      "Iteration 11, Average Loss: 1.527260\n",
      "Iteration 16, Average Loss: 2.129502\n",
      "jmean 0.050736jmax 0.157923\n",
      "hmean 0.469969hmax 0.720784\n",
      "Iteration 21, Average Loss: 4.052370\n",
      "Iteration 26, Average Loss: 3.868772\n",
      "Iteration 31, Average Loss: 4.451977\n",
      "Iteration 36, Average Loss: 3.949155\n",
      "jmean 0.056015jmax 0.233780\n",
      "hmean 0.742110hmax 0.989750\n",
      "Iteration 41, Average Loss: 3.087864\n",
      "Iteration 46, Average Loss: 3.028316\n",
      "Iteration 51, Average Loss: 3.138477\n",
      "Iteration 56, Average Loss: 3.322171\n",
      "jmean 0.056449jmax 0.271072\n",
      "hmean 0.752157hmax 0.989667\n",
      "Iteration 61, Average Loss: 4.016346\n",
      "Iteration 66, Average Loss: 4.096690\n",
      "Iteration 71, Average Loss: 1.622535\n",
      "Layer 1, Epoch 1: Loss 1.919953\n",
      "Output shape after layer 1: (7188, 64)\n",
      "[RBM] Epoch 1/2 \tAverage Loss: 1.919953\n",
      "[RBM] Layer 1, Epoch 1: Reconstruction Error = 0.129593\n",
      "\n",
      "jmean 0.062872jmax 0.444385\n",
      "hmean 0.892880hmax 1.082571\n",
      "Iteration 1, Average Loss: -0.981081\n",
      "Iteration 6, Average Loss: -2.037851\n",
      "Iteration 11, Average Loss: -3.898361\n",
      "Iteration 16, Average Loss: -5.743194\n",
      "jmean 0.064630jmax 0.852975\n",
      "hmean 0.916225hmax 1.089625\n",
      "Iteration 21, Average Loss: -7.854145\n",
      "Iteration 26, Average Loss: -9.244076\n",
      "Iteration 31, Average Loss: -9.500887\n",
      "Iteration 36, Average Loss: -9.978807\n",
      "jmean 0.061230jmax 0.989795\n",
      "hmean 0.842374hmax 1.079044\n",
      "Iteration 41, Average Loss: -8.950392\n",
      "Iteration 46, Average Loss: -8.043549\n",
      "Iteration 51, Average Loss: -8.079338\n",
      "Iteration 56, Average Loss: -8.030774\n",
      "jmean 0.074577jmax 1.031690\n",
      "hmean 0.914220hmax 1.062873\n",
      "Iteration 61, Average Loss: -11.197848\n",
      "Iteration 66, Average Loss: -13.813422\n",
      "Iteration 71, Average Loss: -14.243812\n",
      "Layer 1, Epoch 2: Loss -14.374142\n",
      "Output shape after layer 1: (7188, 64)\n",
      "[RBM] Epoch 2/2 \tAverage Loss: -14.374142\n",
      "[RBM] Layer 1, Epoch 2: Reconstruction Error = 0.101599\n",
      "\n",
      "[DBN] Pre-training finished\n",
      "\n",
      "[DBN] Pre-training RBM layer 2/2: 128 -> 256\n",
      "[DBN] Pre-training start:\n",
      "jmean 0.039043jmax 0.133332\n",
      "hmean 0.052295hmax 0.096623\n",
      "Iteration 1, Average Loss: 0.206437\n",
      "Iteration 6, Average Loss: 6.150082\n",
      "Iteration 11, Average Loss: 0.629198\n",
      "Iteration 16, Average Loss: -0.850214\n",
      "jmean 0.056974jmax 0.880386\n",
      "hmean 0.720710hmax 0.990932\n",
      "Iteration 21, Average Loss: -7.059684\n",
      "Iteration 26, Average Loss: -10.381324\n",
      "Iteration 31, Average Loss: -12.030088\n",
      "Iteration 36, Average Loss: -12.648964\n",
      "jmean 0.055456jmax 1.095523\n",
      "hmean 0.858920hmax 1.089709\n",
      "Iteration 41, Average Loss: -15.946118\n",
      "Iteration 46, Average Loss: -18.839617\n",
      "Iteration 51, Average Loss: -22.039132\n",
      "Iteration 56, Average Loss: -27.163048\n",
      "jmean 0.063703jmax 0.995906\n",
      "hmean 0.952034hmax 1.072673\n",
      "Iteration 61, Average Loss: -31.481791\n",
      "Iteration 66, Average Loss: -35.942318\n",
      "Iteration 71, Average Loss: -42.085434\n",
      "Layer 2, Epoch 1: Loss -41.984946\n",
      "Output shape after layer 2: (7188, 128)\n",
      "[RBM] Epoch 1/2 \tAverage Loss: -41.984946\n",
      "[RBM] Layer 2, Epoch 1: Reconstruction Error = 0.049238\n",
      "\n",
      "jmean 0.066976jmax 1.090380\n",
      "hmean 0.956150hmax 1.079892\n",
      "Iteration 1, Average Loss: -0.912961\n",
      "Iteration 6, Average Loss: -3.774891\n",
      "Iteration 11, Average Loss: -5.047554\n",
      "Iteration 16, Average Loss: -9.870611\n",
      "jmean 0.077297jmax 1.091752\n",
      "hmean 0.801989hmax 1.080073\n",
      "Iteration 21, Average Loss: -13.667176\n",
      "Iteration 26, Average Loss: -19.216535\n",
      "Iteration 31, Average Loss: -26.385567\n",
      "Iteration 36, Average Loss: -28.019048\n",
      "jmean 0.079180jmax 0.991730\n",
      "hmean 0.762256hmax 1.056680\n",
      "Iteration 41, Average Loss: -29.786092\n",
      "Iteration 46, Average Loss: -36.694455\n",
      "Iteration 51, Average Loss: -39.144214\n",
      "Iteration 56, Average Loss: -41.960743\n",
      "jmean 0.074403jmax 1.091172\n",
      "hmean 0.720603hmax 1.080047\n",
      "Iteration 61, Average Loss: -44.787003\n",
      "Iteration 66, Average Loss: -47.943546\n",
      "Iteration 71, Average Loss: -53.699763\n",
      "Layer 2, Epoch 2: Loss -55.228745\n",
      "Output shape after layer 2: (7188, 128)\n",
      "[RBM] Epoch 2/2 \tAverage Loss: -55.228745\n",
      "[RBM] Layer 2, Epoch 2: Reconstruction Error = 0.073544\n",
      "\n",
      "[DBN] Pre-training finished\n",
      "Starting fine-tuning...\n",
      "Built fine-tuning network with 5 layers\n",
      "Input size: 64\n",
      "Output size: 10\n",
      "Fine-tuning Epoch 10/100, Loss: 2.2593, Accuracy: 16.67%\n",
      "Fine-tuning Epoch 20/100, Loss: 2.0198, Accuracy: 25.11%\n",
      "Fine-tuning Epoch 30/100, Loss: 1.5785, Accuracy: 43.11%\n",
      "Fine-tuning Epoch 40/100, Loss: 1.3488, Accuracy: 51.15%\n",
      "Fine-tuning Epoch 50/100, Loss: 1.0904, Accuracy: 63.52%\n",
      "Fine-tuning Epoch 60/100, Loss: 0.8281, Accuracy: 73.76%\n",
      "Fine-tuning Epoch 70/100, Loss: 0.6733, Accuracy: 78.62%\n",
      "Fine-tuning Epoch 80/100, Loss: 0.5759, Accuracy: 81.68%\n",
      "Fine-tuning Epoch 90/100, Loss: 0.5010, Accuracy: 84.20%\n",
      "Fine-tuning Epoch 100/100, Loss: 0.4415, Accuracy: 86.62%\n",
      "Fine-tuning completed. \n",
      "Fine-tuning network training accuracy: 86.62%\n",
      "Fine-tuning network testing accuracy: 0.8436\n",
      "\n",
      "=== Mode 2: Pipline Classifier ===\n",
      "\n",
      "[DBN] Pre-training RBM layer 1/1: 64 -> 128\n",
      "[DBN] Pre-training start:\n",
      "jmean 0.039183jmax 0.124623\n",
      "hmean 0.050424hmax 0.100000\n",
      "Iteration 1, Average Loss: 0.074235\n",
      "Iteration 6, Average Loss: 2.046808\n",
      "Iteration 11, Average Loss: 1.708867\n",
      "Iteration 16, Average Loss: 2.631867\n",
      "jmean 0.047480jmax 0.199253\n",
      "hmean 0.481025hmax 0.724737\n",
      "Iteration 21, Average Loss: 3.535841\n",
      "Iteration 26, Average Loss: 4.386495\n",
      "Iteration 31, Average Loss: 3.014538\n",
      "Iteration 36, Average Loss: 4.060535\n",
      "jmean 0.055012jmax 0.227642\n",
      "hmean 0.700256hmax 1.038924\n",
      "Iteration 41, Average Loss: 4.104102\n",
      "Iteration 46, Average Loss: 2.488781\n",
      "Iteration 51, Average Loss: 2.439187\n",
      "Iteration 56, Average Loss: 1.301805\n",
      "jmean 0.064660jmax 0.249330\n",
      "hmean 0.812438hmax 1.089437\n",
      "Iteration 61, Average Loss: 1.307625\n",
      "Iteration 66, Average Loss: -1.803894\n",
      "Iteration 71, Average Loss: -3.302906\n",
      "Layer 1, Epoch 1: Loss -4.426772\n",
      "Output shape after layer 1: (7188, 64)\n",
      "[RBM] Epoch 1/2 \tAverage Loss: -4.426772\n",
      "[RBM] Layer 1, Epoch 1: Reconstruction Error = 0.102817\n",
      "\n",
      "jmean 0.073577jmax 0.353402\n",
      "hmean 0.939558hmax 1.088123\n",
      "Iteration 1, Average Loss: -1.147211\n",
      "Iteration 6, Average Loss: -1.857075\n",
      "Iteration 11, Average Loss: -5.354403\n",
      "Iteration 16, Average Loss: -4.689729\n",
      "jmean 0.071550jmax 0.571357\n",
      "hmean 0.925435hmax 1.069796\n",
      "Iteration 21, Average Loss: -6.594655\n",
      "Iteration 26, Average Loss: -8.449656\n",
      "Iteration 31, Average Loss: -9.014905\n",
      "Iteration 36, Average Loss: -11.245511\n",
      "jmean 0.066877jmax 0.761620\n",
      "hmean 0.903172hmax 1.056312\n",
      "Iteration 41, Average Loss: -10.610130\n",
      "Iteration 46, Average Loss: -10.291445\n",
      "Iteration 51, Average Loss: -13.975312\n",
      "Iteration 56, Average Loss: -14.780731\n",
      "jmean 0.068179jmax 0.953587\n",
      "hmean 0.913989hmax 1.035637\n",
      "Iteration 61, Average Loss: -19.392936\n",
      "Iteration 66, Average Loss: -18.470584\n",
      "Iteration 71, Average Loss: -17.673266\n",
      "Layer 1, Epoch 2: Loss -17.172633\n",
      "Output shape after layer 1: (7188, 64)\n",
      "[RBM] Epoch 2/2 \tAverage Loss: -17.172633\n",
      "[RBM] Layer 1, Epoch 2: Reconstruction Error = 0.120380\n",
      "\n",
      "[DBN] Pre-training finished\n",
      "Training pipline classifier: random_forest\n",
      "Classifier training accuracy: 1.0000\n",
      "Classifier testing accuracy: 0.9009\n",
      "\n",
      "=== Feature Extraction ===\n",
      "Original shape: (1797, 64)\n",
      "Feature shape: (1797, 128)\n"
     ]
    }
   ],
   "source": [
    "from supervised_dbn_digits import load_data, RBMVisualizer, SupervisedDBNClassification\n",
    "\n",
    "def demonstrate_both_modes():\n",
    "    \"\"\"演示两种模式的使用\"\"\"\n",
    "    # 加载数据\n",
    "    X_train, X_test, y_train, y_test = load_data(plot_img=False)\n",
    "    \n",
    "    # 模式1: 使用微调网络\n",
    "    print(\"=== Mode 1: Fine-tuning Network ===\")\n",
    "    dbn_fine_tune = SupervisedDBNClassification(\n",
    "        hidden_layers_structure=[128, 256],\n",
    "        learning_rate_rbm=0.1,\n",
    "        n_epochs_rbm=2,\n",
    "        batch_size=64,\n",
    "        fine_tuning=True,  # 启用微调\n",
    "        learning_rate=0.1,\n",
    "        n_iter_backprop=100,\n",
    "        l2_regularization=1e-4,\n",
    "        activation_function='sigmoid',\n",
    "        verbose=True,\n",
    "        # plot_img=False,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    dbn_fine_tune.fit(X_train, y_train)\n",
    "    acc = dbn_fine_tune.score(X_test, y_test)\n",
    "    print(f\"Fine-tuning network testing accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # 模式2: 使用下游分类器\n",
    "    print(\"\\n=== Mode 2: Pipline Classifier ===\")\n",
    "    dbn_classifier = SupervisedDBNClassification(\n",
    "        hidden_layers_structure=[128],\n",
    "        # hidden_layers_structure=[128, 256],\n",
    "        learning_rate_rbm=0.1,\n",
    "        n_epochs_rbm=2,\n",
    "        batch_size=64,\n",
    "        fine_tuning=False,  # 使用分类器\n",
    "        # 选项: logistic, svm, random_forest\n",
    "        classifier_type = 'random_forest', \n",
    "        clf_C=500,\n",
    "        clf_iter=1000,\n",
    "        verbose=True,\n",
    "        # plot_img=False,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    dbn_classifier.fit(X_train, y_train)\n",
    "    acc2 = dbn_classifier.score(X_test, y_test)\n",
    "    print(f\"Classifier testing accuracy: {acc2:.4f}\")\n",
    "    \n",
    "    # 特征提取示例\n",
    "    print(\"\\n=== Feature Extraction ===\")\n",
    "    features = dbn_classifier.transform(X_test)\n",
    "    print(f\"Original shape: {X_test.shape}\")\n",
    "    print(f\"Feature shape: {features.shape}\")\n",
    "    \n",
    "    return dbn_fine_tune, dbn_classifier\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model1, model2 = demonstrate_both_modes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21841a98-0736-4d75-9e68-20ad3f234398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5219bace-0a47-4eca-bfa7-c4deaa21120b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
